{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib, re, pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Vendor Name',\n",
       " u'Department/Facility',\n",
       " u'Contract Number',\n",
       " u'Current Contract Amount',\n",
       " u'Spending to Date',\n",
       " u'Contract Start Date',\n",
       " u'Contract End Date',\n",
       " u'Contract Description',\n",
       " u'Contract Type',\n",
       " u'Original Contract Approved/Filed Date']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first page\n",
    "first = urllib.urlopen('http://wwe2.osc.state.ny.us/transparency/contracts/contractresults.cfm?PageNum_rsContract=1&sb=a&a=Z0000&ac=&v=%28Enter+Vendor+Name%29&vo=B&cn=&c=-1&m1=0&y1=0&m2=0&y2=0&am=0&b=Search&order=VENDOR_NAME&sort=ASC')\n",
    "soup = BeautifulSoup(first, \"lxml\")\n",
    "\n",
    "colnames = []\n",
    "th = soup.find_all('th')\n",
    "for name in th:\n",
    "    colnames.append(name.find_all('a')[1].text)\n",
    "colnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parse ALL Historical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There're 2613 pages (1-2613) so far [April 8th]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Order those records by different column. Remember to change their urls.\n",
    "def url(page_num):\n",
    "    base1 = 'http://wwe2.osc.state.ny.us/transparency/contracts/contractresults.cfm?PageNum_rsContract='\n",
    "    base2 = '&sb=a&a=Z0000&ac=&v=%28Enter+Vendor+Name%29&vo=B&cn=&c=-1&m1=0&y1=0&m2=0&y2=0&am=0&b=Search&order=VENDOR_NAME&sort=ASC' \n",
    "    return base1 + str(page_num) + base2\n",
    "# Test\n",
    "# url(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=colnames)\n",
    "pages_range = range(1,2) # Choose the range of pages you want to parse; all -> range(1, 2614)\n",
    "\n",
    "for i in pages_range:\n",
    "    page_url = url(i)\n",
    "    page = urllib.urlopen(page_url)\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    page_data = []\n",
    "    rows = soup.find('table').find_all('tr')\n",
    "    for row in rows[1:]:\n",
    "        cols = row.find_all('td')\n",
    "        cols = [ele.text.strip() for ele in cols]\n",
    "        page_data.append([ele for ele in cols if ele]) # why if TRUE?\n",
    "    temp = pd.DataFrame(page_data, columns=colnames)\n",
    "    df = df.append(temp, ignore_index=True)\n",
    "    \n",
    "# output should be a csv generated by pd.DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Find Upate or Append data (from website)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ID means 'Contract Number'<br/>Historical data (eg. csv fetched before) & web parser for 'new' data in [Open Book New York](http://wwe2.osc.state.ny.us/transparency/contracts/contractresults.cfm?PageNum_rsContract=1&sb=a&a=Z0000&ac=&v=%28Enter+Vendor+Name%29&vo=B&cn=&c=-1&m1=0&y1=0&m2=0&y2=0&am=0&b=Search&order=VENDOR_NAME&sort=ASC) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# test Update function: change value by hand\n",
    "df.iloc[0,0] = None\n",
    "print df.iloc[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# input: historical data\n",
    "# test: copy of historical data for manipulatin\n",
    "test = df.copy()\n",
    "id_exist = map(lambda x: str(x), test['Contract Number'].values)\n",
    "test['hash'] = test.apply(lambda x: hash(tuple(x)), axis = 1)\n",
    "\n",
    "update = []\n",
    "append = []\n",
    "pages_range = range(1,3) #Choose the range of pages you want to check, then append or update\n",
    "\n",
    "for i in pages_range:\n",
    "    page_url = url(i)\n",
    "    page = urllib.urlopen(page_url)\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    rows = soup.find('table').find_all('tr')\n",
    "    for row in rows[1:]:\n",
    "        cols = row.find_all('td')\n",
    "        content = [ele.text.strip() for ele in cols]\n",
    "        if not str(content[2]) in id_exist:\n",
    "            append.append(content)\n",
    "        else:\n",
    "            target_id = str(content[2])\n",
    "            old_hash = test[test['Contract Number'] == target_id]['hash'].values\n",
    "            new_hash = hash(tuple(content)) #hash function: hash value then for comparing two rows\n",
    "            if old_hash != new_hash:\n",
    "                update.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test Update function: validate result \n",
    "# print \n",
    "# df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Update (update & append) Historical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# append\n",
    "df = df.append(pd.DataFrame(append, columns=colnames), ignore_index=True)\n",
    "# update\n",
    "# For Update part, append updated records at the end of historical dataframe, \n",
    "# then drop historical record (duplication by  'Contract Number') fetched before.\n",
    "df = df.append(pd.DataFrame(update, columns=colnames), ignore_index=True)\n",
    "df = df.drop_duplicates(subset=['Contract Number'], keep='last') \n",
    "# reset index. (not same with index in historical dataframe, if there're some updated records)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -> it works!\n",
    "# df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for test\n",
    "df.to_csv('./cusp_hackathon.csv',index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df2 = pd.read_csv(\"./cusp_hackathon.csv\", index_col=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
